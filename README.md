# ğŸš€ Efficient LoRA Fine-Tuning on KLUE-RoBERTa  

This repository contains an optimized **Low-Rank Adaptation (LoRA) fine-tuning** implementation for **KLUE-RoBERTa**, utilizing **8-bit quantization** for efficient training on limited computational resources such as Google Colab.  

---

## ğŸ“Œ Features
âœ… **LoRA-based Fine-Tuning**: Reduces trainable parameters for efficient optimization.  
âœ… **8-bit Quantization**: Lowers memory usage while maintaining performance.  
âœ… **Sentiment Analysis Task**: Trained on the NSMC (Korean Movie Reviews) dataset.  
âœ… **Colab-Ready**: Designed to run on Google Colab with minimal modifications.  

---

## ğŸ“‚ File Structure
lora-finetuning â”œâ”€â”€ lora_finetune.py # Main script for fine-tuning KLUE-RoBERTa â”œâ”€â”€ requirements.txt # List of required dependencies â”œâ”€â”€ README.md # Project documentation

ğŸ“ To-Do
- Evaluate on additional Korean NLP tasks
- Explore other lightweight tuning methods (e.g., QLoRA)
- Deploy as an API using FastAPI


# ğŸš€llm-alignment-lora
KLUE-RoBERTa ê¸°ë°˜ LoRA + 8-bit ì–‘ìí™” ì ìš© LLM

# KLUE-RoBERTa + LoRA Fine-Tuning (8-bit ì–‘ìí™”)
Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ëŸ‰í™”ëœ KLUE-RoBERTa LoRA Fine-Tuning ì½”ë“œì…ë‹ˆë‹¤.

## ğŸ“Œ ì£¼ìš” ë‚´ìš©
âœ… **LoRA (Low-Rank Adaptation)** ì ìš©
âœ… **8-bit ì–‘ìí™”** (`bitsandbytes` í™œìš©)ë¡œ ê²½ëŸ‰í™”
âœ… **KLUE-RoBERTa** ëª¨ë¸ ê¸°ë°˜ Fine-Tuning
âœ… **Colab ì‹¤í–‰ ê°€ëŠ¥ (ë¬´ë£Œ í™˜ê²½ ì§€ì›)**

## ì‹¤í–‰ ë°©ë²•
1. Colabì—ì„œ notebook ì‹¤í–‰
2. 'requirements.txt' íŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ 'lora_finetune.py' ì‹¤í–‰

## ğŸ”— ì°¸ê³  ìë£Œ
- [LoRA ë…¼ë¬¸](https://arxiv.org/abs/2106.09685)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
