# llm-alignment-lora
KLUE-RoBERTa ê¸°ë°˜ LoRA + 8-bit ì–‘ìí™” ì ìš© LLM

# KLUE-RoBERTa + LoRA Fine-Tuning (8-bit ì–‘ìí™”)
Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ëŸ‰í™”ëœ KLUE-RoBERTa LoRA Fine-Tuning ì½”ë“œì…ë‹ˆë‹¤.

## ì£¼ìš” ë‚´ìš©
- **LoRA (Low-Rank Adaptation)** ì ìš©
- **8-bit ì–‘ìí™”** (`bitsandbytes` í™œìš©)ë¡œ ê²½ëŸ‰í™”
- **KLUE-RoBERTa** ëª¨ë¸ ê¸°ë°˜ Fine-Tuning
- **Colab ì‹¤í–‰ ê°€ëŠ¥ (ë¬´ë£Œ í™˜ê²½ ì§€ì›)**

## ì‹¤í–‰ ë°©ë²•
1. Colabì—ì„œ notebook ì‹¤í–‰
2. 'requirements.txt' íŒ¨í‚¤ì§€ ì„¤ì¹˜ í›„ 'lora_finetune.py' ì‹¤í–‰

## ğŸ”— ì°¸ê³  ìë£Œ
- [LoRA ë…¼ë¬¸](https://arxiv.org/abs/2106.09685)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
